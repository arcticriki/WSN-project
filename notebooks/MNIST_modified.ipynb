{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arcticriki/WSN-project/blob/master/notebooks/MNIST_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJsUjs19_v63"
      },
      "source": [
        "# MNIST with SciKit-Learn and skorch\n",
        "\n",
        "This notebooks shows how to define and train a simple Neural-Network with PyTorch and use it via skorch with SciKit-Learn.\n",
        "\n",
        "<table align=\"left\"><td>\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
        "</td><td>\n",
        "<a target=\"_blank\" href=\"https://github.com/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zmIlvxI_v68"
      },
      "source": [
        "**Note**: If you are running this in [a colab notebook](https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb), we recommend you enable a free GPU by going:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**\n",
        "\n",
        "If you are running in colab, you should install the dependencies and download the dataset by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qYGNO2S_v6_"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Installation on Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    subprocess.run(['python', '-m', 'pip', 'install', 'skorch' , 'torch'])\n",
        "except ImportError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gj0pvjxT_v7G"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPz6Bjqw_v7H"
      },
      "source": [
        "## Loading Data\n",
        "Using SciKit-Learns ```fetch_openml``` to load MNIST data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mwpfASvc_v7J"
      },
      "outputs": [],
      "source": [
        "mnist = fetch_openml('mnist_784', as_frame=False, cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9Pt2JKyb_v7K",
        "outputId": "b62e801b-b471-42e7-b98d-a94e3bc1f2d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "mnist.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV0ehb52_v7L"
      },
      "source": [
        "## Preprocessing Data\n",
        "\n",
        "Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image. Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel.<br />\n",
        "The above ```featch_mldata``` method to load MNIST returns ```data``` and ```target``` as ```uint8``` which we convert to ```float32``` and ```int64``` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F2v_Fwne_v7M"
      },
      "outputs": [],
      "source": [
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_yHJarZ_v7N"
      },
      "source": [
        "To avoid big weights that deal with the pixel values from between [0, 255], we scale `X` down. A commonly used range is [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8RirCOTr_v7O"
      },
      "outputs": [],
      "source": [
        "X /= 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rohyp3d1_v7P",
        "outputId": "297c4ab1-b6be-422c-ece4-1efb8549749e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float32(0.0), np.float32(1.0))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X.min(), X.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyUlsu0V_v7Q"
      },
      "source": [
        "Note: data is not normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gILlsHJS_v7R"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jGpA4v4u_v7R"
      },
      "outputs": [],
      "source": [
        "assert(X_train.shape[0] + X_test.shape[0] == mnist.data.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "plXmcsp2_v7b",
        "outputId": "669a4eb8-3be4-47ad-e2c3-cb81fa460f18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((52500, 784), (52500,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EKEvbuP_v7c"
      },
      "source": [
        "### Print a selection of training images and their labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C9muXJPC_v7d"
      },
      "outputs": [],
      "source": [
        "def plot_example(X, y):\n",
        "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
        "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
        "        plt.subplot(151 + i)\n",
        "        plt.imshow(img)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h2-R1-Df_v7e",
        "outputId": "b3f17fb3-3479-4510-d376-bddac30e61e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_example(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQvC-rWf_v7f"
      },
      "source": [
        "## Build Neural Network with PyTorch\n",
        "Simple, fully connected neural network with one hidden layer. Input layer has 784 dimensions (28x28), hidden layer has 98 (= 784 / 8) and output layer 10 neurons, representing digits 0 - 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5PG7R0W8_v7f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NUjWrGBP_v7g"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTHWWPcvTBgc",
        "outputId": "8b423002-0e80-414f-85b9-ea8c65000f8d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gVCW8F3N_v7g"
      },
      "outputs": [],
      "source": [
        "mnist_dim = X.shape[1]\n",
        "hidden_dim = int(mnist_dim/8)\n",
        "output_dim = len(np.unique(mnist.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ShYicHlv_v7h",
        "outputId": "e97c89b7-19f6-4124-e2cf-6f30031e7c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 98, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "mnist_dim, hidden_dim, output_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeVnFhBS_v7i"
      },
      "source": [
        "A Neural network in PyTorch's framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xxtli0l__v7j"
      },
      "outputs": [],
      "source": [
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim=mnist_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            output_dim=output_dim,\n",
        "            dropout=0.5,\n",
        "    ):\n",
        "        super(ClassifierModule, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X, **kwargs):\n",
        "        X = F.relu(self.hidden(X))\n",
        "        X = self.dropout(X)\n",
        "        X = F.softmax(self.output(X), dim=-1)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlEHSwjt_v7k"
      },
      "source": [
        "skorch allows to use PyTorch's networks in the SciKit-Learn setting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s0aDatqN_v7l"
      },
      "outputs": [],
      "source": [
        "from skorch import NeuralNetClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dOrCbBjk_v7m"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "net = NeuralNetClassifier(\n",
        "    ClassifierModule,\n",
        "    max_epochs=20,\n",
        "    lr=0.1,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8i_gnvPi_v7m",
        "outputId": "e63d40c4-a47b-47fa-ae2c-d74e57b73774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_acc    valid_loss     dur\n",
            "-------  ------------  -----------  ------------  ------\n",
            "      1        \u001b[36m0.8387\u001b[0m       \u001b[32m0.8800\u001b[0m        \u001b[35m0.4174\u001b[0m  1.5601\n",
            "      2        \u001b[36m0.4332\u001b[0m       \u001b[32m0.9103\u001b[0m        \u001b[35m0.3133\u001b[0m  0.8785\n",
            "      3        \u001b[36m0.3612\u001b[0m       \u001b[32m0.9233\u001b[0m        \u001b[35m0.2684\u001b[0m  0.8923\n",
            "      4        \u001b[36m0.3233\u001b[0m       \u001b[32m0.9309\u001b[0m        \u001b[35m0.2317\u001b[0m  0.8917\n",
            "      5        \u001b[36m0.2938\u001b[0m       \u001b[32m0.9353\u001b[0m        \u001b[35m0.2173\u001b[0m  0.8810\n",
            "      6        \u001b[36m0.2738\u001b[0m       \u001b[32m0.9390\u001b[0m        \u001b[35m0.2039\u001b[0m  0.8960\n",
            "      7        \u001b[36m0.2600\u001b[0m       \u001b[32m0.9454\u001b[0m        \u001b[35m0.1868\u001b[0m  1.3529\n",
            "      8        \u001b[36m0.2427\u001b[0m       \u001b[32m0.9484\u001b[0m        \u001b[35m0.1757\u001b[0m  1.1685\n",
            "      9        \u001b[36m0.2362\u001b[0m       \u001b[32m0.9503\u001b[0m        \u001b[35m0.1683\u001b[0m  1.0849\n",
            "     10        \u001b[36m0.2226\u001b[0m       \u001b[32m0.9512\u001b[0m        \u001b[35m0.1621\u001b[0m  1.1712\n",
            "     11        \u001b[36m0.2184\u001b[0m       \u001b[32m0.9529\u001b[0m        \u001b[35m0.1565\u001b[0m  0.8763\n",
            "     12        \u001b[36m0.2090\u001b[0m       \u001b[32m0.9541\u001b[0m        \u001b[35m0.1508\u001b[0m  0.8522\n",
            "     13        \u001b[36m0.2067\u001b[0m       \u001b[32m0.9570\u001b[0m        \u001b[35m0.1446\u001b[0m  0.8722\n",
            "     14        \u001b[36m0.1978\u001b[0m       \u001b[32m0.9570\u001b[0m        \u001b[35m0.1412\u001b[0m  0.8739\n",
            "     15        \u001b[36m0.1923\u001b[0m       \u001b[32m0.9582\u001b[0m        \u001b[35m0.1392\u001b[0m  0.8665\n",
            "     16        \u001b[36m0.1889\u001b[0m       0.9582        \u001b[35m0.1342\u001b[0m  0.8749\n",
            "     17        \u001b[36m0.1855\u001b[0m       \u001b[32m0.9612\u001b[0m        \u001b[35m0.1297\u001b[0m  0.8699\n",
            "     18        \u001b[36m0.1786\u001b[0m       \u001b[32m0.9613\u001b[0m        \u001b[35m0.1266\u001b[0m  0.8708\n",
            "     19        \u001b[36m0.1728\u001b[0m       \u001b[32m0.9615\u001b[0m        \u001b[35m0.1250\u001b[0m  0.8879\n",
            "     20        \u001b[36m0.1698\u001b[0m       0.9613        \u001b[35m0.1248\u001b[0m  0.8649\n"
          ]
        }
      ],
      "source": [
        "net.fit(X_train, y_train);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c3iyCKu_v7m"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "D7rdey0s_v7n"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b9B8Zd6e_v7n"
      },
      "outputs": [],
      "source": [
        "y_pred = net.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LCWachuM_v7o",
        "outputId": "b083ba1e-d80a-4f34-9aa9-25d97bb67e7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9631428571428572"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpRyfXveTi95",
        "outputId": "68503256-3fed-49c5-b31d-9681d22a2dfd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.module_.forward(torch.tensor(X_test[0], device=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBkc0_c6Tu2U",
        "outputId": "13062d40-d0df-4537-a4f7-ac2ad7caef9c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.8163e-08, 1.2802e-07, 1.7493e-04, 1.5174e-03, 8.6505e-10, 1.8852e-04,\n",
              "        6.6384e-09, 9.1162e-08, 9.9812e-01, 1.1959e-06], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eRga6AV_v7o"
      },
      "source": [
        "An accuracy of about 96% for a network with only one hidden layer is not too bad.\n",
        "\n",
        "Let's take a look at some predictions that went wrong:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "XddXR1_a_v7p"
      },
      "outputs": [],
      "source": [
        "error_mask = y_pred != y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hXlNTrlt_v7q",
        "outputId": "9cb4af6a-5ee2-48f8-c87e-5a040ed63e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_example(X_test[error_mask], y_pred[error_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2GsBaxH_v7r"
      },
      "source": [
        "# Convolutional Network\n",
        "PyTorch expects a 4 dimensional tensor as input for its 2D convolution layer. The dimensions represent:\n",
        "* Batch size\n",
        "* Number of channel\n",
        "* Height\n",
        "* Width\n",
        "\n",
        "As initial batch size the number of examples needs to be provided. MNIST data has only one channel. As stated above, each MNIST vector represents a 28x28 pixel image. Hence, the resulting shape for PyTorch tensor needs to be (x, 1, 28, 28)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Bwsaz88X_v7r"
      },
      "outputs": [],
      "source": [
        "XCnn = X.reshape(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Go0Yz8xl_v7s",
        "outputId": "42b030bb-b65d-494a-afbd-47f9a43a0c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "XCnn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Rt8ETXGa_v7t"
      },
      "outputs": [],
      "source": [
        "XCnn_train, XCnn_test, y_train, y_test = train_test_split(XCnn, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-jQm85il_v7t",
        "outputId": "93476c89-b7bd-4d22-86db-981b732ee012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((52500, 1, 28, 28), (52500,))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "XCnn_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YdQ-ISvb_v7u"
      },
      "outputs": [],
      "source": [
        "class Cnn(nn.Module):\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super(Cnn, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.conv2_drop = nn.Dropout2d(p=dropout)\n",
        "        self.fc1 = nn.Linear(1600, 100) # 1600 = number channels * width * height\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "        self.fc1_drop = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "\n",
        "        # flatten over channel, height and width = 1600\n",
        "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
        "\n",
        "        x = torch.relu(self.fc1_drop(self.fc1(x)))\n",
        "        x = torch.softmax(self.fc2(x), dim=-1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WJrWyFfb_v7u"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "cnn = NeuralNetClassifier(\n",
        "    Cnn,\n",
        "    max_epochs=10,\n",
        "    lr=0.002,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KRtHfgg8_v7u",
        "outputId": "2df2811e-14ee-49e2-ffbf-05c7ca7b7d0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_acc    valid_loss     dur\n",
            "-------  ------------  -----------  ------------  ------\n",
            "      1        \u001b[36m0.4321\u001b[0m       \u001b[32m0.9717\u001b[0m        \u001b[35m0.0896\u001b[0m  1.9076\n",
            "      2        \u001b[36m0.1651\u001b[0m       \u001b[32m0.9785\u001b[0m        \u001b[35m0.0674\u001b[0m  1.3967\n",
            "      3        \u001b[36m0.1371\u001b[0m       \u001b[32m0.9821\u001b[0m        \u001b[35m0.0571\u001b[0m  1.3914\n",
            "      4        \u001b[36m0.1139\u001b[0m       \u001b[32m0.9850\u001b[0m        \u001b[35m0.0493\u001b[0m  1.6259\n",
            "      5        \u001b[36m0.1027\u001b[0m       \u001b[32m0.9862\u001b[0m        \u001b[35m0.0443\u001b[0m  1.6154\n",
            "      6        \u001b[36m0.0976\u001b[0m       \u001b[32m0.9866\u001b[0m        \u001b[35m0.0411\u001b[0m  1.3970\n",
            "      7        \u001b[36m0.0888\u001b[0m       0.9862        0.0424  1.3925\n",
            "      8        \u001b[36m0.0868\u001b[0m       \u001b[32m0.9879\u001b[0m        \u001b[35m0.0390\u001b[0m  1.3914\n",
            "      9        \u001b[36m0.0818\u001b[0m       0.9878        \u001b[35m0.0390\u001b[0m  1.4100\n",
            "     10        \u001b[36m0.0762\u001b[0m       \u001b[32m0.9887\u001b[0m        \u001b[35m0.0371\u001b[0m  1.4327\n"
          ]
        }
      ],
      "source": [
        "cnn.fit(XCnn_train, y_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "p6V7wyzb_v7v"
      },
      "outputs": [],
      "source": [
        "y_pred_cnn = cnn.predict(XCnn_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZSyiZ3p6_v7v",
        "outputId": "f2c283ca-7ea2-483c-e666-008ce97dd337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9882285714285715"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "accuracy_score(y_test, y_pred_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npZlem33_v7v"
      },
      "source": [
        "An accuracy of >98% should suffice for this example!\n",
        "\n",
        "Let's see how we fare on the examples that went wrong before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2oWL0xGC_v7w",
        "outputId": "879d5dba-4699-4a42-a3c6-e8987ebc1297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7674418604651163"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "accuracy_score(y_test[error_mask], y_pred_cnn[error_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8239U9fF_v7w"
      },
      "source": [
        "Over 70% of the previously misclassified images are now correctly identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5vU1SXeV_v7x",
        "outputId": "18c58d18-64a1-4bde-8186-465c776e8bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_example(X_test[error_mask], y_pred_cnn[error_mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "h-tIl3el_v7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373886be-a5fc-474c-b8a7-88d4a68f5692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-e18358f52827>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = cnn.module_.forward(torch.tensor(batch, device=device))\n",
            "<ipython-input-63-e18358f52827>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = cnn.module_.forward(torch.tensor(batch, device=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy with CNN features: 0.9885142857142857\n"
          ]
        }
      ],
      "source": [
        "# prompt: Write a code snippet that 1. Will use the CNN above to extract features 2. Use such features in a sklearn pipeline to train a logistic regression 3. compute accuracy to see how it compares with the other models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Extract features using the trained CNN\n",
        "cnn_features_train = []\n",
        "for i in range(0, len(XCnn_train), 100):  # Process in batches to avoid memory issues\n",
        "    batch = XCnn_train[i:i+100]\n",
        "    with torch.no_grad():\n",
        "        features = cnn.module_.forward(torch.tensor(batch, device=device))\n",
        "        cnn_features_train.extend(features.cpu().numpy())\n",
        "cnn_features_train = np.array(cnn_features_train)\n",
        "\n",
        "\n",
        "cnn_features_test = []\n",
        "for i in range(0, len(XCnn_test), 100):\n",
        "    batch = XCnn_test[i:i+100]\n",
        "    with torch.no_grad():\n",
        "        features = cnn.module_.forward(torch.tensor(batch, device=device))\n",
        "        cnn_features_test.extend(features.cpu().numpy())\n",
        "cnn_features_test = np.array(cnn_features_test)\n",
        "\n",
        "# Create and train a logistic regression model using the extracted features\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression(max_iter=1000)) # Increased max_iter for convergence\n",
        "])\n",
        "\n",
        "pipeline.fit(cnn_features_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred_lr = pipeline.predict(cnn_features_test)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "print(f\"Logistic Regression accuracy with CNN features: {accuracy_lr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Repeat what you did above, but instead of the CNN built above, use another famous model like Resnet or Alexnet downloaded from huggingface transformers\n",
        "\n",
        "import numpy as np\n",
        "#!pip install transformers\n",
        "\n",
        "from transformers import ResNetModel\n",
        "import torch\n",
        "\n",
        "# Assuming XCnn_train and XCnn_test are defined as in the previous code\n",
        "\n",
        "# Load pre-trained ResNet model from Hugging Face Transformers\n",
        "resnet_model = ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n",
        "resnet_model.to(device) # Move to GPU if available\n",
        "\n",
        "# Repeat the grayscale image 3 times to create a 3-channel input\n",
        "XCnn_train_3ch = XCnn_train.repeat(1, 3, 1, 1) # Repeat along the channel dimension\n",
        "XCnn_test_3ch = XCnn_test.repeat(1, 3, 1, 1)\n",
        "\n",
        "XCnn_train_3ch = torch.tensor(XCnn_train_3ch, device=device)\n",
        "XCnn_test_3ch = torch.tensor(XCnn_test_3ch, device=device)\n",
        "\n",
        "# Extract features using the ResNet model\n",
        "resnet_features_train = []\n",
        "for i in range(0, len(XCnn_train_3ch), 100):\n",
        "    batch = XCnn_train_3ch[i:i+100].to(device)\n",
        "    with torch.no_grad():\n",
        "      outputs = resnet_model(batch)\n",
        "      # Extract features from the average of the last hidden state\n",
        "      features = outputs.last_hidden_state.mean(dim=[1,2]).cpu().numpy()\n",
        "      resnet_features_train.extend(features)\n",
        "\n",
        "resnet_features_train = np.array(resnet_features_train)\n",
        "\n",
        "\n",
        "resnet_features_test = []\n",
        "for i in range(0, len(XCnn_test_3ch), 100):\n",
        "    batch = XCnn_test_3ch[i:i+100].to(device)\n",
        "    with torch.no_grad():\n",
        "      outputs = resnet_model(batch)\n",
        "      # Extract features from the average of the last hidden state\n",
        "      features = outputs.last_hidden_state.mean(dim=[1,2]).cpu().numpy()\n",
        "      resnet_features_test.extend(features)\n",
        "\n",
        "resnet_features_test = np.array(resnet_features_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwUzuL8SVi6h",
        "outputId": "0e8c6cc3-d4b9-4ebc-f178-c279c675a1c7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-f5a89ccc2789>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  XCnn_train_3ch = torch.tensor(XCnn_train_3ch, device=device)\n",
            "<ipython-input-59-f5a89ccc2789>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  XCnn_test_3ch = torch.tensor(XCnn_test_3ch, device=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_features_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgRbPHr7Xe-R",
        "outputId": "6bb65067-56b4-4f91-a174-f58878b8472d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52500, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the features to 2D before fitting\n",
        "resnet_features_train_2d = resnet_features_train.reshape(resnet_features_train.shape[0], -1)\n",
        "resnet_features_test_2d = resnet_features_test.reshape(resnet_features_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "I0ZFF2mKXrU6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train a logistic regression model using the extracted features\n",
        "pipeline = Pipeline([\n",
        "       ('scaler', StandardScaler()),  # Add StandardScaler for feature scaling\n",
        "       ('clf', LogisticRegression(max_iter=1000))\n",
        "   ])\n",
        "\n",
        "pipeline.fit(resnet_features_train_2d, y_train)\n",
        "\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred_lr = pipeline.predict(resnet_features_test_2d)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "print(f\"Logistic Regression accuracy with ResNet features: {accuracy_lr}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5vBTrIAW9wy",
        "outputId": "fb9118a7-be51-4724-e443-fdf74f81cc30"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy with ResNet features: 0.24097142857142856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aksQ1jMBWYnu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd97b8bffa4d3737e84826bc3d37be3046061822757ce35137ab82ad4c5a2016"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}